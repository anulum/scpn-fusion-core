# ─────────────────────────────────────────────────────────────────────
# SCPN Fusion Core — FNO Training on QLKNN-Oracle Spatial Data
# © 1998–2026 Miroslav Sotek. All rights reserved.
# ─────────────────────────────────────────────────────────────────────
"""
Train the JAX FNO on (equilibrium, transport_field) spatial pairs
generated by the QLKNN MLP oracle.

Prerequisites
-------------
1. Train MLP:  python tools/train_neural_transport_qlknn.py
2. Gen data:   python tools/generate_fno_qlknn_spatial.py

Usage
-----
    python tools/train_fno_qlknn_spatial.py
    python tools/train_fno_qlknn_spatial.py --epochs 200 --modes 16 --width 64
"""

from __future__ import annotations

import argparse
import json
import logging
import sys
import time
from pathlib import Path

import numpy as np

logger = logging.getLogger(__name__)

REPO_ROOT = Path(__file__).resolve().parents[1]
DEFAULT_DATA_DIR = REPO_ROOT / "data" / "fno_qlknn_spatial"
DEFAULT_OUTPUT = REPO_ROOT / "weights" / "fno_turbulence_jax.npz"


def train_fno(
    data_dir: Path,
    output_path: Path,
    modes: int = 16,
    width: int = 64,
    epochs: int = 200,
    lr: float = 1e-3,
    batch_size: int = 32,
    seed: int = 42,
) -> None:
    """Train FNO on spatial transport data."""
    import jax
    import jax.numpy as jnp
    from jax import random, grad, jit, vmap

    devices = jax.devices()
    gpu = any(d.platform == "gpu" for d in devices)
    print(f"JAX backend: {'GPU' if gpu else 'CPU'} ({devices[0]})")

    if not gpu:
        jax.config.update("jax_platform_name", "cpu")

    # Load data
    train_data = np.load(data_dir / "train.npz")
    val_data = np.load(data_dir / "val.npz")

    X_train = jnp.array(train_data["X"])  # (N, 64, 64)
    Y_train = jnp.array(train_data["Y"])
    X_val = jnp.array(val_data["X"])
    Y_val = jnp.array(val_data["Y"])

    grid_size = X_train.shape[1]
    n_train = len(X_train)
    print(f"Data: train={n_train}, val={len(X_val)}, grid={grid_size}x{grid_size}")

    # Initialize FNO parameters
    key = random.PRNGKey(seed)

    def init_params(key):
        keys = random.split(key, 10)
        return {
            "w1_real": random.normal(keys[0], (width, width, modes, modes)) * 0.02,
            "w1_imag": random.normal(keys[1], (width, width, modes, modes)) * 0.02,
            "b1": jnp.zeros(width),
            "linear1": random.normal(keys[2], (width, width)) * 0.02,
            "lift_w": random.normal(keys[3], (1, width)) * 0.1,
            "lift_b": jnp.zeros(width),
            "fc1": random.normal(keys[4], (width, 128)) * 0.05,
            "fc1_b": jnp.zeros(128),
            "fc2": random.normal(keys[5], (128, 1)) * 0.05,
            "fc2_b": jnp.zeros(1),
        }

    params = init_params(key)

    @jit
    def fno_forward(params, x):
        """Forward pass: (grid, grid) -> (grid, grid)."""
        # Lift: (G, G) -> (G, G, width)
        x = x[..., None]  # (G, G, 1)
        h = x @ params["lift_w"] + params["lift_b"]  # (G, G, width)

        # Spectral convolution layer
        h_fft = jnp.fft.rfft2(h, axes=(0, 1))  # (G, G//2+1, width)
        M = modes
        # Truncate to modes
        h_low = h_fft[:M, :M, :]  # (M, M, width)

        # Complex spectral multiplication per channel
        w_complex = params["w1_real"] + 1j * params["w1_imag"]  # (width, width, M, M)
        # Einsum: modes x width -> width
        out_fft = jnp.zeros_like(h_fft)
        # Simplified: spectral conv in truncated mode space
        for c_out in range(min(width, 8)):  # limit channels for speed
            for c_in in range(min(width, 8)):
                out_fft = out_fft.at[:M, :M, c_out].add(
                    h_low[:, :, c_in] * w_complex[c_in, c_out, :M, :M]
                )

        h_spec = jnp.fft.irfft2(out_fft, s=(grid_size, grid_size), axes=(0, 1))

        # Skip connection
        h_skip = h @ params["linear1"]

        h = jax.nn.gelu(h_spec + h_skip + params["b1"])

        # Project to scalar field: (G, G, width) -> (G, G)
        h = jax.nn.relu(h @ params["fc1"] + params["fc1_b"])
        h = jax.nn.softplus(h @ params["fc2"] + params["fc2_b"])
        return h.squeeze(-1)  # (G, G)

    @jit
    def loss_fn(params, x_batch, y_batch):
        preds = vmap(lambda x: fno_forward(params, x))(x_batch)
        # Relative L2 loss
        diff_sq = jnp.sum((preds - y_batch) ** 2, axis=(1, 2))
        norm_sq = jnp.sum(y_batch ** 2, axis=(1, 2))
        return jnp.mean(jnp.sqrt(diff_sq / jnp.maximum(norm_sq, 1e-8)))

    @jit
    def relative_l2_metric(params, x, y):
        preds = vmap(lambda xi: fno_forward(params, xi))(x)
        return jnp.sqrt(
            jnp.sum((preds - y) ** 2) / jnp.maximum(jnp.sum(y ** 2), 1e-8)
        )

    grad_fn = jit(grad(loss_fn))

    # Adam optimizer
    adam_m = {k: jnp.zeros_like(v) for k, v in params.items()}
    adam_v = {k: jnp.zeros_like(v) for k, v in params.items()}
    t = 0

    best_val = float("inf")
    best_params = None
    n_batches = max(1, n_train // batch_size)
    t0 = time.monotonic()

    for epoch in range(epochs):
        lr_t = lr * 0.5 * (1 + np.cos(np.pi * epoch / epochs))

        key, sk = random.split(key)
        perm = random.permutation(sk, n_train)
        X_shuf = X_train[perm]
        Y_shuf = Y_train[perm]

        epoch_loss = 0.0
        for b in range(n_batches):
            x_b = X_shuf[b*batch_size : (b+1)*batch_size]
            y_b = Y_shuf[b*batch_size : (b+1)*batch_size]

            grads = grad_fn(params, x_b, y_b)
            t += 1

            for k in params:
                g = jnp.clip(grads[k], -1.0, 1.0)
                adam_m[k] = 0.9 * adam_m[k] + 0.1 * g
                adam_v[k] = 0.999 * adam_v[k] + 0.001 * g ** 2
                m_hat = adam_m[k] / (1 - 0.9 ** t)
                v_hat = adam_v[k] / (1 - 0.999 ** t)
                params[k] = params[k] - lr_t * m_hat / (jnp.sqrt(v_hat) + 1e-8)

            epoch_loss += float(loss_fn(params, x_b, y_b))
        epoch_loss /= n_batches

        val_l2 = float(relative_l2_metric(params, X_val, Y_val))
        if val_l2 < best_val:
            best_val = val_l2
            best_params = {k: np.array(v) for k, v in params.items()}

        if epoch % 20 == 0 or epoch == epochs - 1:
            elapsed = time.monotonic() - t0
            print(f"  Epoch {epoch:4d}/{epochs} | "
                  f"train_rel_l2={epoch_loss:.4f} | val_rel_l2={val_l2:.4f} | "
                  f"best={best_val:.4f} | lr={lr_t:.2e} | {elapsed:.0f}s")

    print(f"\nTraining complete. Best val relative L2: {best_val:.4f}")

    # Verification gate
    print("\n=== VERIFICATION GATE ===")
    if best_val >= 0.30:
        print(f"  FAIL: val_relative_l2 = {best_val:.4f} >= 0.30")
        print("  Try: --epochs 500 --lr 5e-4 --modes 24 --width 128")
        sys.exit(1)
    elif best_val >= 0.10:
        print(f"  WARN: val_relative_l2 = {best_val:.4f} >= 0.10 (acceptable but not ideal)")
    else:
        print(f"  PASS: val_relative_l2 = {best_val:.4f} < 0.10")

    # Save
    output_path.parent.mkdir(parents=True, exist_ok=True)
    np.savez(output_path, **best_params)
    print(f"  Saved weights to {output_path}")

    # Save metrics
    metrics = {
        "val_relative_l2": float(best_val),
        "epochs": epochs,
        "modes": modes,
        "width": width,
        "grid_size": grid_size,
        "n_train": n_train,
        "training_time_s": time.monotonic() - t0,
        "data_source": "QLKNN-MLP oracle spatial pairs",
    }
    metrics_path = output_path.with_suffix(".metrics.json")
    metrics_path.write_text(json.dumps(metrics, indent=2), encoding="utf-8")
    print(f"  Metrics saved to {metrics_path}")
    print("\n=== VERIFICATION PASSED ===")


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--data-dir", type=Path, default=DEFAULT_DATA_DIR)
    parser.add_argument("--output", type=Path, default=DEFAULT_OUTPUT)
    parser.add_argument("--modes", type=int, default=16)
    parser.add_argument("--width", type=int, default=64)
    parser.add_argument("--epochs", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--batch-size", type=int, default=32)
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    train_fno(args.data_dir, args.output, args.modes, args.width,
              args.epochs, args.lr, args.batch_size, args.seed)


if __name__ == "__main__":
    main()
